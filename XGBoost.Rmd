---
title: "XGBoost"
author: "KN Mnisi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

XGBoost, also known as eXtreme Gradient Boosting, is a popular open-source software library for gradient boosting. This framework builds an ensemble of weak models to create a more robust and accurate final model.

Full Model (Using all the explanatory variables)
________________________________________________

Loading Packages needed.
```{r}
library(readxl)
library(xgboost)
library(caret)
library(dplyr)
library(ggplot2)
```

Loading the pre-processed data dataset.
```{r}
data <- read_excel("Clean_AmesHousing.xlsx")
```

Removing unnecessary variables
```{r}
data <- subset(data, select = -c(PID, Order))
```

XGBoost only allows numerical data therefore converting categorical variables in the data dataset to numeric format.
```{r}
 categorical_var <- names(data %>%
  select_if(is.character))
for (col in categorical_var) {
  data[[col]] <- as.numeric(factor(data[[col]]))
}
```

Setting a seed value for reproductibility.
```{r}
set.seed(1)
```

Splitting the dataset into a 70:30 training and testing sets to allow for evaluating the model's performance on unseen data, which helps in building more robust models less prone to overfitting. Furthermore, transforming the train and test sets into matrices is done for ensuring compatibility with machine learning algorithms that typically require data in matrix format for training and prediction.
dataset
```{r}
target <- "SalePrice"
y <- data[[target]]
data <- data[, !colnames(data) %in% target]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- data[train_indices, ]
train_target <- y[train_indices]
test_data <- data[-train_indices, ]
test_target <- y[-train_indices]
```

Calculating the mean selling price for comparison purposes.
```{r}
mean <- mean(y)
```

Training the XGBoost model
```{r}
set.seed(1)
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1600,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```

Making predictions using the xgboost full model.
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
comb <- data.frame(obs = test_target, pred = round(predictions,0))
print(head(comb))
```

Evaluating the xgboost full model using the test set.
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)
cat("The Coeficient of Determination is: ", R2,"%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Visualising the predicted vs the observed values.
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 1, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size
# Set margins to create more space for the legend on the right
par(mar = c(5, 4, 4, 6)) # c(bottom, left, top, right)
# Plot the line graph
plot(x, comb$obs, type = "l", col = "purple", lwd = 2, xlab = "Index", 
     ylab = "Sale Price", main = "Observed vs Predicted")
# Add the line for predicted values
lines(x, comb$pred, col = "pink", lwd = 2)
# Adding a Legend
legend_x <- max(x) + 1
legend_y <- mean(comb$obs)
legend(x = "right", y = legend_y, legend = c("Observed", "Predicted"), 
       col = c("purple", "pink"), lty = 1, lwd = 2, xpd = TRUE, cex = 0.7, inset = c(-0.2, 0))
```

Reduced Model (Using Top 40 variables)
______________________________________

Getting the top40 variables based on the XGBoost feature Selection.
```{r}
importance_df <- data.frame(xgb.importance(colnames(train_data), 
                                           model = xgb_model))
top40 <- importance_df[1:40,1]
top40 <- data[, top40]
names(top40)
top40$SalePrice <- y
top40 <- top40[, !colnames(top40) %in% target]
```

Visualizing the top 40 variables recommended by the XGBoost feature selection.
```{r}
new <- importance_df[1:40,1:2]
new <- new[order(-new$Gain), ]
# Create the bar chart
ggplot(new, aes(y = reorder(Feature, Gain), x = Gain)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "Gain", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

Splitting the dataset into a 70:30 training and testing sets to allow for evaluating the model's performance on unseen data, which helps in building more robust models less prone to overfitting. Furthermore, transforming the train and test sets into matrices is done for ensuring compatibility with machine learning algorithms that typically require data in matrix format for training and prediction.
```{r}
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- top40[train_indices, ]
train_target <- y[train_indices]
test_data <- top40[-train_indices, ]
test_target <- y[-train_indices]
```

Training the XGBoost reduced model.
```{r}
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1500,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```
Making predictions using the xgboost reduced model.
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
(comb <- data.frame(obs = test_target, pred = round(predictions,0)))
```

Evaluating the reduced model using the test set.
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
r_squared <- round(cor(predictions, test_target)^2 *100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)
cat("The Coeficient of Determination is: ", R2,"%",r_squared, "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Visualizing the predicted vs the observed house prices.
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size
par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)
# Plot the line graph
plot(x, comb$obs, type = "l", col = "blue", lwd = 2, xlab = "Index",
     ylab = "Sale Price", main = "XGBoost Observed vs Predicted Prices")
# Add the line for predicted values
lines(x, comb$pred, col = "red", lwd = 2)
# Add a legend
legend(x= "right", y = mean(comb$pred), legend = c("Observed", "Predicted"), 
       col = c("blue", "red"), lty = 1, lwd = 2, xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))
```
