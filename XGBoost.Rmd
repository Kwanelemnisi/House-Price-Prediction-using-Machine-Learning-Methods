---
title: "XGBoost"
author: "KN Mnisi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Packages
```{r}
library(readxl)
library(xgboost)
library(caret)
library(dplyr)
```
Loading the pre-processed data dataset inclusive of outliers
```{r}
data <- read_excel("Clean_AmesHousing.xlsx")
```
The model was fitted initially, but the presence of outliers has resulted in an
increased mean absolute error. Consequently, to address this issue, the model
will be refitted using a dataset from which the outliers have been removed.
```{r}
# Calculate z-scores
z_scores <- scale(data$SalePrice)

# Set the threshold for outliers
threshold <- 3

# Remove outliers
data <- data[abs(z_scores) < threshold, ]
```
Remove unnecessary variables
```{r}
data <- subset(data, select = -c(PID, Order))
```
XGBoost only allows numerical data. Convert categorical variables in the data
dataset to numeric format
```{r}
 categorical_var <- names(data %>%
  select_if(is.character))

for (col in categorical_var) {
  data[[col]] <- as.numeric(factor(data[[col]]))
}
```

```{r}
# Set the target variable
target <- "SalePrice"
# Create the target variable
y <- data[[target]]
# Create the data for modeling (removing the target variable)
data <- data[, !colnames(data) %in% target]
```

```{r}
mean <- mean(y)
```

```{r}
# Split the data into training and testing sets
set.seed(1)
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- data[train_indices, ]
train_target <- y[train_indices]
test_data <- data[-train_indices, ]
test_target <- y[-train_indices]
```
Train the XGBoost model
```{r}
set.seed(1)
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1600,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```

Make predictions on the test data
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
comb <- data.frame(obs = test_target, pred = round(predictions,0))
print(head(comb))
```
Evaluate the model
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2,"%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```
Visualisation


```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 1, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

# Set margins to create more space for the legend on the right
par(mar = c(5, 4, 4, 6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "purple", lwd = 2, xlab = "Index", 
     ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$pred, col = "pink", lwd = 2)

legend_x <- max(x) + 1
legend_y <- mean(comb$obs)

# Set xpd to TRUE to allow drawing outside the plot area
# par(xpd = TRUE)
# Add a legend just below the x-axis with minimized box size

legend(x = "right", y = legend_y, legend = c("Observed", "Predicted"), 
       col = c("purple", "pink"), lty = 1, lwd = 2, xpd = TRUE, cex = 0.7, inset = c(-0.2, 0))

```

Building a XGBoost model using the top40 variables based on the XGBoost feature 
Selection
```{r}
importance_df <- data.frame(xgb.importance(colnames(train_data), 
                                           model = xgb_model))
top40 <- importance_df[1:40,1]
top40 <- data[, top40]
names(top40)
top40$SalePrice <- y
```
```{r}
library(ggplot2)
new <- importance_df[1:40,1:2]
new <- new[order(-new$Gain), ]
# Create the bar chart
ggplot(new, aes(y = reorder(Feature, Gain), x = Gain)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "Gain", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```


Create the data for modeling (removing the target variable)
```{r}
top40 <- top40[, !colnames(top40) %in% target]
```

Split the data into training and testing sets
```{r}
set.seed(1)
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- top40[train_indices, ]
train_target <- y[train_indices]
test_data <- top40[-train_indices, ]
test_target <- y[-train_indices]
```
Train the XGBoost model
```{r}
set.seed(1)
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1500,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```
Make predictions on the test data
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
comb <- data.frame(obs = test_target, pred = round(predictions,0))
print(head(comb))
```
Evaluate the model
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
r_squared <- round(cor(predictions, test_target)^2 *100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2,"%",r_squared, "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```
Visualisation
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "blue", lwd = 2, xlab = "Index",
     ylab = "Sale Price", main = "XGBoost Observed vs Predicted Prices")

# Add the line for predicted values
lines(x, comb$pred, col = "red", lwd = 2)

# Add a legend
legend(x= "right", y = mean(comb$pred), legend = c("Observed", "Predicted"), 
       col = c("blue", "red"), lty = 1, lwd = 2, xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))

```
Findings: 

After fitting two XGBoost models, a full model and a reduced model comprising
the top 40 features selected through a feature selection process, a thorough
comparison was conducted to determine the preferred model. The evaluation of 
these models involved examining various performance metrics, with a focus on 
their predictive accuracy and error rates.

Starting with the coefficient of determination (R-squared), both models 
demonstrated a commendable level of explanatory power, achieving an identical 
value of 92%. This implies that they can explain 92% of the variance present in
the target variable, a crucial aspect in assessing model performance.

To delve further into their predictive capabilities, the mean squared error 
(MSE) was computed. The reduced model exhibited a lower MSE of 394,419,635, 
surpassing the full model's MSE of 413,918,676. Consequently, the reduced model
showcases a more precise prediction of the target variable compared to its
counterpart.

Additionally, the root mean squared error (RMSE) was utilized to provide a more
interpretable measure of the prediction errors. Remarkably, the reduced model
achieved a lower RMSE of 19,860, outperforming the full model, which yielded
an RMSE of 20,345. This indicates that the reduced model's predictions tend to 
be closer to the true values of the target variable, reflecting its superior 
accuracy.

Turning to the mean absolute error (MAE), both models yielded similar results,
with the reduced model achieving a slightly lower MAE of 13,045 compared to the
full model's MAE of 13,115. Moreover, when considering the percentage of MAE
relative to the overall mean, the reduced model recorded a slight advantage at
7.44%, while the full model stood at 7.48%. These metrics provide further 
evidence of the reduced model's ability to minimize the magnitude of prediction 
errors.

Considering the comprehensive analysis of the performance metrics, it becomes
evident that the reduced model emerges as the preferred choice. Not only does 
it maintain the same coefficient of determination as the full model, reflecting
a robust explanatory capacity, but it also exhibits superior predictive accuracy
in terms of lower MSE, RMSE, and a slightly reduced MAE. Thus, the reduced model
stands out as the optimal selection based on the conducted observations and
analysis.