---
title: "DECIPHERING THE INTRICATE RELATIONSHIPS WITHIN THE HOUSING MARKET"
author: "KN Mnisi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Packages
```{r}
library(doBy)
library(dplyr)
library(readr)
library(corrplot)
library(RColorBrewer)
library(mice)
library(FactoMineR)
library(factoextra)
library(caret)
library(randomForest)
library(tibble)
library(ggplot2)
```

Importing the Ames Housing dataset.
```{r}
AmesHousing <- read_csv("AmesHousing.csv")
```
_________________________________________________________________________________________________________
SECTION 1: DATA ENGINEERING
_________________________________________________________________________________________________________

Calculating the number of NA (null) values and the percentage of NA values in each column. 
```{r}
Total_Na_Values <- colSums(is.na(AmesHousing))
Percentage <- floor(Total_Na_Values / nrow(AmesHousing) * 100)
NA_Counts <- cbind.data.frame(Total_Na_Values, Percentage)
```

Calculating the total number of variables that has NA values. 
```{r}
nonzero_rows <- sum(rowSums(NA_Counts != 0) > 0)
cat("There are", nonzero_rows, "variables containing NA values namely (Sorted in descending order):","\n")
nonzero_observations <- NA_Counts[rowSums(NA_Counts != 0) > 0, ]
(sorted_observations <- nonzero_observations[order(-nonzero_observations$Total_Na_Values), ])
```

There are numerous ways to deal with NA observations in a dataset.
*  Removing missing values: You can remove rows or columns with missing values from
   the dataset using the na.omit() or complete.cases() functions, respectively.
*  Replacing missing values: You can replace missing values with a value based on the 
   type of data. For example, you can replace missing numeric values with the mean or 
   median of the column, or replace missing categorical values with the mode etc.
*  Using imputation techniques: You can use more advanced imputation techniques to fill 
   in missing values. There are several R packages that provide imputation functions, 
   such as mice, missForest, and Amelia. 
   
Within the Ames dataset, there are five variables that contain a substantial number of missing values. These variables and their corresponding percentages of missing values are as follows: Alley (99%), Fire Place Qu (96%), Pool QC (93%), Fence (80%), Misc Feature (48%), and Lot Frontage (16%). Notably, the first four variables can be substituted with descriptive categories to indicate the absence of certain features. Specifically, "no alley access" (NAA), "no fireplace" (NFP), "no pool" (NP), and "no miscellaneous features" (NMF) respectively.  
```{r}
AmesHousing$Alley <- replace(AmesHousing$Alley, is.na(AmesHousing$Alley), "NAA")
AmesHousing$`Fireplace Qu` <- replace(AmesHousing$`Fireplace Qu`, 
                                      is.na(AmesHousing$`Fireplace Qu`), "NFP")
AmesHousing$`Pool QC` <- replace(AmesHousing$`Pool QC`, is.na(AmesHousing$`Pool QC`), "NP")
AmesHousing$Fence <- replace(AmesHousing$Fence, is.na(AmesHousing$Fence), "NF")
AmesHousing$`Misc Feature` <- replace(AmesHousing$`Misc Feature`, 
                                      is.na(AmesHousing$`Misc Feature`), "NMF")
```

The lot frontage variable cannot be replaced using such descriptive categories (as above) as there is no available information to determine the meaning behind the missing values. As a result, the imputation approach will be used to handle the missing values in the variable.
a) Removing spaces from variables for effective imputation.
```{r}
names(AmesHousing)[5] <- "LotFrontage"
names(AmesHousing)[6] <- "LotArea"
names(AmesHousing)[9] <- "LotShape"
names(AmesHousing)[4] <- "MSZoning"
names(AmesHousing)[12] <-"LotConfig"
```
b) Imputation is the done to address missing values in the "Lot Frontage" variable using the mice package. It uses predictive mean matching as the imputation method, running up to 50 iterations to generate one imputed dataset. The result is extracted in a long format, including both the original data and the imputed values, ensuring a comprehensive dataset with no missing values.
```{r}
vars <- c("LotFrontage", "LotArea", "LotShape", "Neighborhood", "MSZoning", "Street", "LotConfig")
dataset <- AmesHousing[, vars]
imp_model <- mice(dataset, method = "pmm", m = 1, maxit = 50)
imputed_data <- complete(imp_model, action = "long", include = TRUE)
AmesHousing$LotFrontage <- imputed_data$LotFrontage[2931:5860]
```
c) Returning the variables be its original format
```{r}
names(AmesHousing)[5] <- "Lot Frontage"
names(AmesHousing)[6] <- "Lot Area"
names(AmesHousing)[9] <- "Lot Shape"
names(AmesHousing)[4] <- "MS Zoning"
names(AmesHousing)[12] <- "Lot Config"
```

The presence of NA values in the garage-related variables, namely garage type (nominal), garage cond (ordinal), garage qual (nominal), garage finish (nominal), and garage yr blt (discrete), indicates the absence of a garage in the respective houses. It is worth noting that these variables collectively exhibit a consistent occurrence rate of 5% (157 values for every variable) for NA values. To address the missing values in the nominal and ordinal variables, a suitable approach is to substitute the NA values with the "no garage" (NG) category. This maintains consistency in the dataset and facilitates further analysis and interpretation. Regarding the garage yr built variable, the presence of null values signifies that the properties do not have garages. Since substituting these values with meaningful information is not feasible, a dummy variable named Has Garage is created. This variable takes the value 0 to represent "no garage" and 1 to indicate "garage present."
a) Removing the garage year built variable from the dataset. 
```{r}
AmesHousing <- subset(AmesHousing, select = -`Garage Yr Blt`)
```
b) Creating a dummy variable "Has Garage" that takes the value 0 to represent "no garage" and 1 to indicate "garage present".
```{r}
AmesHousing$`Has Garage` <- NA
AmesHousing$`Has Garage` <- ifelse(AmesHousing$`Garage Area` == 0, 0, 1)
```
c) Assign "NG" (No garage) to variables when `Has Garage` is 0
```{r}
AmesHousing$`Garage Type`[AmesHousing$`Has Garage` == 0] <- "NG"
AmesHousing$`Garage Cond`[AmesHousing$`Has Garage` == 0] <- "NG"
AmesHousing$`Garage Finish`[AmesHousing$`Has Garage` == 0] <- "NG"
AmesHousing$`Garage Qual`[AmesHousing$`Has Garage` == 0] <- "NG"
```

Among the basement-related variables in the dataset, several variables exhibit a consistent occurrence of 2% NA values. These variables include "bsmt exposure" (with 83 NA values), "bsmt qual" (with 80 NA values), "bsmt cond" (with 80 NA values), "bsmtfin type 1" (with 80 NA values), and "bsmtfin type 2" (with 81 NA values). It is worth noting that the number of NA values in each variable is not uniform, making it unwise to assume and replace all NA values with a generic category such as "no basement." Given the relatively insignificant percentage of NA values in these variables, it is reasonable to omit the NA values during analysis. This omission would lead to the elimination of the NA values in other basement-related variables, namely "bsmt full bath," "bsmt half bath," "bsmtfin sf 1," "bsmtfin sf 2," "bsmt unf sf," and "total bsmt sf."
Assing "NB" (No basement) to variables when `Total Bsmt SF` is 0.
```{r}
AmesHousing$`Bsmt Exposure`[AmesHousing$`Total Bsmt SF` == 0] <- "NB"
AmesHousing$`Bsmt Cond`[AmesHousing$`Total Bsmt SF` == 0] <- "NB"
AmesHousing$`Bsmt Qual`[AmesHousing$`Total Bsmt SF` == 0] <- "NB"
AmesHousing$`BsmtFin Type 1`[AmesHousing$`Total Bsmt SF` == 0] <- "NB"
AmesHousing$`BsmtFin Type 2`[AmesHousing$`Total Bsmt SF` == 0] <- "NB"
```

The "Mas Vnr Type" and "Mas Vnr Area" variables in the dataset have two missing values each, signifying the absence or unavailability of information regarding the type or area measurement of the masonry veneer for certain properties. Therefore, it is appropriate to remove these missing values from the dataset. Similarly, the "Electrical" variable has one missing value, suggesting missing or unavailable information about the type of electrical system in a specific house. This missing value can also be omitted from the analysis. Additionally, the "Garage Cars" and "Garage Area" variables each have one missing value, representing missing information related to the number of cars the garage can accommodate and the area of the garage, respectively. Omitting these missing values ensures data integrity and enables analysis based on complete and reliable information for the respective variables.

All other NA values present in the dataset, including the variables already discussed above that have NA observations remaining, is be omitted. 
```{r}
AmesHousing <- na.omit(AmesHousing)
```
Now this is the cleaned dataset that will be used for the rest of the study.

______________________________________________________________________________________________________________
SECTION 2: DATA EXPLORATION
______________________________________________________________________________________________________________

Visualising a Box Plot.
```{r}
options(scipen = 999)
par(mar = c(5, 4, 4, 2)
    cex.axis = 0.6)
par(las = 1, yaxs = "i")
boxplot(AmesHousing$SalePrice,
        main = "Box Plot of Sale Price",
        ylab = "Sale Price",
        ylim = c(0,755000))
```
The box plot provides valuable insights into the distribution of house sale prices. It is apparent that the boxplot is skewed to the right. This skewness implies that the Ames housing market comprises a considerable number of moderately priced houses, while a smaller proportion of the market contains high-priced properties. The median sale price of $160 000 indicates the middle value, with half of the houses sold below this price and the other half above it. Moreover, the interquartile range (IQR) between $129 000 (1st Quartile, Q1) and $213,000 (3rd Quartile, Q3) represents the middle 50% of the data, offering an understanding of the typical range of sale prices. However, there are potential outliers, with the minimum sale price at $12 789 and the maximum at $755 000, indicating a few houses with exceptionally low or high sale prices compared to the majority of the data.

Extracting the outliers of the Sale Price variable from dataset  by using the z-scores approach.
```{r}
z_scores <- scale(AmesHousing$SalePrice)
outliers <- which(abs(z_scores) > 3)
outliers <- AmesHousing[outliers, ]
```
Based on the z-scores approach, a thorough analysis reveals the presence of 44 outliers within the Sale Price variable. While outliers often require careful consideration and treatment in data cleaning processes, this particular study refrains from addressing them during the initial dataset cleaning stage. This decision stems from the fact that the proposed algorithms employed in the study possess the ability to handle outliers, and furthermore, there exists a possibility that these outliers may hold valuable insights or unique characteristics worth exploring further.


Correlation Plot for all numeric variables in the Ames Dataset, excluding the "Order" Variable which is column 1.
```{r}
numeric_cols <- AmesHousing[sapply(AmesHousing, is.numeric)][,-1]
cor_matrix <- cor(numeric_cols)
corrplot(cor_matrix, method = "color",tl.cex = 0.5, col = colorRampPalette(brewer.pal(11, "RdYlBu"))(100))
```
The correlation heatmap reveals crucial insights into the dataset, showing the correlations between variables. Notably, there is a substantial positive correlation of 81.36% between Total Bsmt SF and 1st Flr SF, indicating that as the total basement area increases, so does the area on the first floor. Additionally, the correlation of 80.75% between TotRms AbvGrd and Gr Liv Area suggests that houses with more rooms above ground level tend to have larger ground living areas. Concerning the target variable, SalePrice, it exhibits a strong correlation with two key features. First, the Overall Qual) shows a correlation of 79.93%, implying that higher overall quality in a house is associated with higher sale prices. Second, the Gr Liv Area demonstrates a correlation of 70.86%, indicating that homes with larger ground living areas tend to achieve higher selling prices.


Calculating and analysing the mean sale price for every year. 
```{r}
Year_Sold <- AmesHousing$`Yr Sold`
Sale_Price <- AmesHousing$SalePrice
data <- data.frame(Year_Sold, Sale_Price)
(means <- aggregate(Sale_Price ~ Year_Sold, AmesHousing, mean))
```
The mean sale prices of houses in the Ames housing market from 2006 to 2010 showns an intriguing trend. The market experienced steady growth from 2006 to 2007, with the average sale price increasing from $180 879.60 to $184 728.70. This upswing can be attributed to a growing economy, increased housing demand, and possible inflationary pressures. However, in 2008, the average sale price slightly decreased to $178 587.30, which might be linked to the onset of the global financial crisis during that period, leading to a slowdown in the housing market. Despite the challenging financial crisis, the market exhibited resilience, rebounding to a mean sale price of $181 026.80 in 2009 and maintaining relative stability. In 2010, there was a more significant drop in the mean sale price to $172 323.80, which could be associated with the lingering effects of the financial crisis and the slower recovery process.


Checking how many variables will be optimal to use for our reduced model so that at least 80% of the variation in the response variable is explained. 
a) Creating a temporal dataset which is a duplicate of the AmesHousing dataset and transforming all variables to be numeric variables as it is required by the Principal Component Analysis (PCA) in order to determine the number of components (features) that explain at least 80% of the variation in the dataset
```{r}
pacdata <- AmesHousing
categorical_var <- names(pacdata %>%
                           select_if(is.character))
for (col in categorical_var) {
  pacdata[[col]] <- as.numeric(factor(pacdata[[col]]))
}
```
b) Perform PCA. The PCA (Principal Component Analysis) technique is used to reduce the dimensionality of a dataset while retaining at least a percentage (80% in this case) of its variation. It accomplishes this by transforming the original variables into a set of orthogonal principal components. The results, including principal components, eigenvalues, and variable loadings, are stored in `pca_result`, and graphical output is suppressed with `graph = FALSE` in the function call.
```{r}
pca_result <- PCA(AmesHousing[,!colnames(AmesHousing) %in% "SalePrice"] , graph = FALSE)
```
c) Determining the number of components to retain based on explained variance Finding the number of components that explain at least 80% of the variance
```{r}
explained_variance <- pca_result$eig[, 2]  # Percentage of variance explained
cumulative_variance <- cumsum(explained_variance)
optimal_num_variables <- which(cumulative_variance >= 80)[1]
cat(optimal_num_variables, "must be used for the reduced models in order to explain atleast 80% of the variance", "\n")
```
It has been ascertained by the PAC technique that utilizing 40 variables will yield the desired results, establishing it as the preferred number of features to be used for the reduced models.

______________________________________________________________________________________________________________________
SECTION 3: MACHINE LEARNING TECHNIQUES (ALGORITHMS)
______________________________________________________________________________________________________________________

A seed value of 1 has been utilised to ensure reproducibility. 
```{r}
set.seed(1)
```

Remove the unnecessary variables for this section as they carry no information that affects the Selling Price of houses.
```{r}
AmesHousing <- subset(AmesHousing, select = -c(PID, Order))
```

Splitting the dataset into a 70:30 training and testing sets to allow for evaluating the model's performance on unseen data, which helps in building more robust models less prone to overfitting. Furthermore, transforming the train and test sets into matrices is done for ensuring compatibility with machine learning algorithms that typically require data in matrix format for training and prediction.
dataset
```{r}
response <- "SalePrice"
y <- data[[response]]
x <- data[, !colnames(data) %in% response]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_X <- as.matrix(x[train_indices, ])
train_Y <- y[train_indices]
test_X <- as.matrix(x[-train_indices, ])
test_Y <- y[-train_indices]
```

1. Random Forest --- Random forest is a versatile and powerful machine learning algorithm that 
      utilizes an ensemble learning method to build multiple decision trees and combine their
      predictions for more accurate results.
_________________________________________________________________________________________________
  
a) Full Model (Using all explainatory variables)
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Recalling the cleaned dataset and renaming it "dataRF" for temporal use in the Random Forest section only.
```{r}
dataRF <- AmesHousing
```

Modifying the variable names because the randomForest function is sensitive to special characters.
```{r}
new_names <- gsub(" ", "", names(dataRF))
names(dataRF) <- new_names
names(dataRF)[22] <- "YearRemodORAdd"
names(dataRF)[45] <- "FstFlrSF"
names(dataRF)[46] <- "ScndFlrSF"
names(dataRF)[70] <- "ThreeSsnPorch"
```


Overall Mean of the SalePrice
```{r}
mean <- mean(data$SalePrice)
mean
```


Visualisation of feature importance
```{r}
data.rf <- randomForest(train_Y ~ ., train_X, ntree= 100, keep.forest= TRUE, importance = TRUE)
varImpPlot(data.rf)
```
Predicting the SalePrice using the random forest model
```{r}
predictions <- round(predict(data.rf, test_X),0)
```

Combining the predicted and observed sale price for comparison purposes. 
```{r}
comb <- data.frame(Pred = predictions, Obs = test_Y)
print(head(comb))
```

Calculating the performance measures that will be used for comparison purposes
```{r}
sse <- sum((test_Y - predictions)^2)
ssto <- sum((test_Y - mean(test_Y))^2)
R2 <- round((1 - (sse / ssto))*100,0)
# r_squared <- round(cor(predictions, test_Y)^2 *100,0)
mse <- mean((test_Y - predictions)^2)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2, "%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average value of the data is:",perc,"%", "\n")
```


To visualize the difference between the observed and predicted sale prices, a line plot can be generated. This plot will effectively illustrate the variance between the two variables.
```{r}
x <- seq_along(comb$Obs)
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size
par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)
plot(x, comb$Obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Observed vs Predicted")
lines(x, comb$Pred, col = "red", lwd = 2)
legend(x = "right",y= mean(comb$Obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))
```


b) Reduced Model (Using 40 Variables)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To optimize the random forest model, a reduced version will be fitted using the top 40 variables (as ascertained by the PAC technique). These variables were carefully selected based on the %incNodePurity metric, which emphasizes the quality of split points in the decision tree. Through experimentation, it was discovered that reducing the number of variables further increased the mean absolute error and decreased the R^2 value. Consequently, the decision was made to utilize the top 40 variables, chosen based on their %incNodePurity scores. This approach was favored over using the %IncMSE variables, as they yielded inferior model performance.
```{r}
importance_scoresRF <- importance(data.rf, type=2)
# Convert the matrix to a data frame and include the row names as a variable
importance_dfRF <- as.data.frame(importance_scoresRF) %>%
  rownames_to_column(var = "Feature")
# Sort the data frame by importance scores in descending order
importance_dfRF <- importance_dfRF[order(importance_dfRF$IncNodePurity, decreasing = TRUE), ][1:40,]
```




Creating a side bar graph to visualise the featured and their respective importance scores
ggplot(importance_df, aes(y = reorder(Feature, IncNodePurity), x = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "IncNodePurity Score", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```
It becomes evident when observing the plot that certain key features wield significant influence in
determining a property’s sale price. Ranking these factors in order of importance, the top five influential elements
are as follows: first, the overall quality of the property emerges as the most decisive determinant, indicating
a positive correlation between higher overall quality and elevated sale prices. Second, the ground living area,
representing the total floor space dedicated to living quarters, also exhibits substantial sway over the property’s
value. Third, the year built, reflecting the property’s age, exerts a discernible impact, with newer constructions
generally commanding higher prices. Fourth, the external quality, which assesses the condition of the property’s
exterior, is another crucial factor affecting the sale price. Lastly, the total basement surface area secures a spot
16
among the top five features, implying that properties boasting more extensive basement spaces tend to fetch
higher prices.

Fitting the reduced random forest model

```{r}
top40 <- data.frame(Variables = importance_df[1:40,1])
top40 <- data[, top40$Variables]
names(top40)
top40$SalePrice <- data$SalePrice
```
```{r}
set.seed(1)
# Split the data into training and testing sets (or use cross-validation)
set.seed(1)
target <- "SalePrice"
y <- top40[[target]]
data1 <- top40[, !colnames(top40) %in% target]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_X <- as.matrix(data1[train_indices, ])
train_Y <- y[train_indices]
test_X <- as.matrix(data1[-train_indices, ])
test_Y <- y[-train_indices]
```

```{r}
top40.rf <- randomForest(train_Y ~ ., train_X, ntree= 50, keep.forest= TRUE, importance = TRUE)

```
To predict the SalePrice, the random forest model was fitted using the top 40 features selected based on the %IncMSE metric
```{r}
predictions <- round(predict(top40.rf, test_X),0)
```
Combining the predicted and observed sale price for comparison purposes. 
```{r}
comb <- data.frame(Pred = predictions, Obs = test_Y)
print(head(comb))
```
Calculating the performance measures that will be used for comparison purposes
```{r}
sse <- sum((test_Y - predictions)^2)
ssto <- sum((test_Y - mean(test_Y))^2)
R2 <- round((1 - (sse / ssto))*100,0) 
# r_squared <- round(cor(predictions, test_Y)^2 *100,0)
mse <- mean((test_Y - predictions)^2)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2,"%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```


Generating a line plot to visualize the difference between the observed and predicted sale price. 
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$Obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$Obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$Pred, col = "red", lwd = 2)

# Add a legend
legend(x = "right",y= mean(comb$Obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))

```


2. Support Vector Regression

Load the required package
```{r}
library(kernlab)
library(readxl)
library(dplyr)
library(caret)
library(ggplot2)
```
Load data, remove unnecessary variables and then change categorical data into numeric data
```{r}
data <- read_excel("Clean_AmesHousing.xlsx")
```

```{r}
# Calculate z-scores
z_scores <- scale(data$SalePrice)

# Set the threshold for outliers
threshold <- 3

# Remove outliers
data <- data[abs(z_scores) < threshold, ]
```

```{r}
data <- subset(data, select = -c(PID, Order))
mean <- mean(data$SalePrice)

target_column_index <- 79

features <- data[,-target_column_index]

categorical_var <- names(data %>%
                           select_if(is.character))

for (col in categorical_var) {
  data[[col]] <- as.numeric(factor(data[[col]]))
}
```

Split the data into training and testing sets 
```{r}
set.seed(1)
target <- "SalePrice"
y <- data[[target]]
data1 <- data[, !colnames(data) %in% target]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_X <- as.matrix(data1[train_indices, ])
train_Y <- y[train_indices]
test_X <- as.matrix(data1[-train_indices, ])
test_Y <- y[-train_indices]
```

Fit the full SVR model using the ksvm function
```{r}
svr_model <- ksvm(train_X, train_Y, type="eps-svr", kernel="rbfdot", kpar="automatic")
```

Predictions
```{r}
predictions <- predict(svr_model, test_X)
comb <- data.frame(obs = test_Y, pred = round(predictions,0))
print(head(comb))
```

Evaluate the model
```{r}
r_squared <- round(cor(predictions, test_Y)^2 *100,0)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae/mean)*100,2)

# Print the evaluation metrics
cat("R-squared:", r_squared,"%", "\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Visualisation
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$pred, col = "red", lwd = 2)

# Add a legend
legend(x = "right",y= mean(comb$obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))
```

Performing feature selection in order fit the reduced support vector regression model
```{r}
# Define the 'svmRadial' function for the SVR method
svmRadial_func <- function(x, y, ...){
  kernlab::ksvm(x, y, type = "eps-svr", kernel = "rbfdot", kpar = "automatic", ...)
}

# Create the caretFuncs list with the 'svmRadial' function
caretFuncs <- list(svmRadial = svmRadial_func)

# Feature Selection using Recursive Feature Elimination (RFE)
num_features_to_select <- 40  # Set the desired number of features
ctrl <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_results <- rfe(train_X, train_Y,
                   sizes = num_features_to_select,
                   rfeControl = ctrl,
                   method = "svmRadial")
```


```{r}
top40 <- data.frame(rfe_results$variables[318:357,1:2])
```

Create the bar chart
```{r}
ggplot(top40, aes(y = reorder(var, Overall), x = Overall)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "Overall Score", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

Create the data for modeling (removing the target variable)
```{r}
features <- data[,-target_column_index]
top40 <- top40$var
top40 <- data[, top40]
train_X <- as.matrix(top40[train_indices, ])
test_X <- as.matrix(top40[-train_indices, ])
```

Fit the reduced SVR model using the ksvm function
```{r}
svr_model <- ksvm(train_X, train_Y, type="eps-svr", kernel="rbfdot", kpar="automatic")
```

Predictions
```{r}
predictions <- predict(svr_model, as.matrix(test_X))
comb <- data.frame(obs = test_Y, pred = round(predictions,0))
print(head(comb))
```

Evaluate the reduced model
```{r}
r <- 1 - sum((test_Y - predictions)^2) / sum((test_Y - mean(test_Y))^2)
r_squared <- round(cor(predictions, test_Y)^2 *100,0)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae/mean)*100,2)

# Print the evaluation metrics
cat("R-squared:", r_squared,"%",r, "\n")
cat("RMSE:", rmse, "\n")
cat("MAE:", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")

```

Visualisation
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$pred, col = "red", lwd = 2)

# Add a legend
legend(x = "right",y= mean(comb$obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))

```

3. XGBoost

Packages
```{r}
library(readxl)
library(xgboost)
library(caret)
library(dplyr)
library(ggplot2)
```

Loading the pre-processed data dataset inclusive of outliers
```{r}
data <- read_excel("Clean_AmesHousing.xlsx")
```

The model was fitted initially, but the presence of outliers has resulted in an
increased mean absolute error. Consequently, to address this issue, the model
will be refitted using a dataset from which the outliers have been removed.
```{r}
# Calculate z-scores
z_scores <- scale(data$SalePrice)

# Set the threshold for outliers
threshold <- 3

# Remove outliers
data <- data[abs(z_scores) < threshold, ]
```

Remove unnecessary variables
```{r}
data <- subset(data, select = -c(PID, Order))
```

XGBoost only allows numerical data. Convert categorical variables in the data
dataset to numeric format
```{r}
 categorical_var <- names(data %>%
  select_if(is.character))

for (col in categorical_var) {
  data[[col]] <- as.numeric(factor(data[[col]]))
}
```

```{r}
# Set the target variable
target <- "SalePrice"
# Create the target variable
y <- data[[target]]
# Create the data for modeling (removing the target variable)
data <- data[, !colnames(data) %in% target]
```

```{r}
mean <- mean(y)
```

Split the data into training and testing sets
```{r}
set.seed(1)
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- data[train_indices, ]
train_target <- y[train_indices]
test_data <- data[-train_indices, ]
test_target <- y[-train_indices]
```

Train the XGBoost model
```{r}
set.seed(1)
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1600,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```

Make predictions on the test data
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
comb <- data.frame(obs = test_target, pred = round(predictions,0))
print(head(comb))
```

Evaluate the model
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2,"%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Visualisation
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 1, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

# Set margins to create more space for the legend on the right
par(mar = c(5, 4, 4, 6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "purple", lwd = 2, xlab = "Index", 
     ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$pred, col = "pink", lwd = 2)

legend_x <- max(x) + 1
legend_y <- mean(comb$obs)

# Set xpd to TRUE to allow drawing outside the plot area
# par(xpd = TRUE)
# Add a legend just below the x-axis with minimized box size

legend(x = "right", y = legend_y, legend = c("Observed", "Predicted"), 
       col = c("purple", "pink"), lty = 1, lwd = 2, xpd = TRUE, cex = 0.7, inset = c(-0.2, 0))

```

Fitting a XGBoost model using the top40 variables based on the XGBoost feature 
Selection
```{r}
importance_df <- data.frame(xgb.importance(colnames(train_data), 
                                           model = xgb_model))
top40 <- importance_df[1:40,1]
top40 <- data[, top40]
names(top40)
top40$SalePrice <- y
```

Creating a bar chart to visualise the top40 variables
```{r}
new <- importance_df[1:40,1:2]
new <- new[order(-new$Gain), ]
ggplot(new, aes(y = reorder(Feature, Gain), x = Gain)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "Gain", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

Create the data for modeling (removing the target variable)
```{r}
top40 <- top40[, !colnames(top40) %in% target]
```

Split the data into training and testing sets
```{r}
set.seed(1)
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_data <- top40[train_indices, ]
train_target <- y[train_indices]
test_data <- top40[-train_indices, ]
test_target <- y[-train_indices]
```

Train the reduced XGBoost model
```{r}
set.seed(1)
xgb_model <- xgboost(data = as.matrix(train_data),
                     label = train_target,
                     nrounds = 1500,  # Number of boosting iterations
                     objective = "reg:squarederror",  # Regression objective
                     eta = 0.1,  # Learning rate
                     max_depth = 2,  # Maximum tree depth
                     verbose = FALSE,  # Suppress iteration printing
                     colsample_bytree = 0.8)  # Set seed for reproducibility
```

Make predictions on the test data
```{r}
predictions <- predict(xgb_model, as.matrix(test_data))
comb <- data.frame(obs = test_target, pred = round(predictions,0))
print(head(comb))
```

Evaluate the reduced model
```{r}
mae <- round(mean(abs(predictions - test_target)),0)
sse <- sum((test_target - predictions)^2)
ssto <- sum((test_target - mean(test_target))^2)
R2 <- round((1 - (sse / ssto))*100,0)
r_squared <- round(cor(predictions, test_Y)^2 *100,0)
p <- ncol(test_data)
n <- nrow(test_data)
df <- n - p
mse <- round((sse / df),0)
rmse <- round(sqrt(mse),0)
perc <- round((mae / mean)*100,2)

cat("The Coeficient of Determination is: ", R2,"%",r_squared, "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Visualisation
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size

par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)

# Plot the line graph
plot(x, comb$obs, type = "l", col = "blue", lwd = 2, xlab = "Index",
     ylab = "Sale Price", main = "Observed vs Predicted")

# Add the line for predicted values
lines(x, comb$pred, col = "red", lwd = 2)

# Add a legend
legend(x= "right", y = mean(comb$pred), legend = c("Observed", "Predicted"), 
       col = c("blue", "red"), lty = 1, lwd = 2, xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))

```
