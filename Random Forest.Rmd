---
title: "Random Forest"
author: "KN Mnisi"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Random forest is a versatile and powerful machine learning algorithm that utilizes an ensemble learning method to build multiple decision trees and combine their predictions for more accurate results.

Full Model (Uses all the explanatory variables)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Calling Packages needed.
```{r}
library(readxl)
library(ggplot2)
library(caret)
library(randomForest)
library(tibble)
```

Loading the pre-processed AmesHousing dataset,
```{r}
data <- read_excel("Clean_AmesHousing.xlsx")
```

Modifying the variable names because the randomForest function is sensitive to special characters.
```{r}
new_names <- gsub(" ", "", names(data))
names(data) <- new_names
names(data)[22] <- "YearRemodORAdd"
names(data)[45] <- "FstFlrSF"
names(data)[46] <- "ScndFlrSF"
names(data)[70] <- "ThreeSsnPorch"
```

Removing unnecessary variables.
```{r}
data <- subset(data, select = -c(PID, Order))
```

Overall Mean of the SalePrice
```{r}
(mean <- mean(data$SalePrice))
```

Setting a seed value to ensure reproducibility. 
```{r}
set.seed(1)
```

Splitting the dataset into a 70:30 training and testing sets to allow for evaluating the model's performance on unseen data, which helps in building more robust models less prone to overfitting. Furthermore, transforming the train and test sets into matrices is done for ensuring compatibility with machine learning algorithms that typically require data in matrix format for training and prediction.
dataset
```{r}
target <- "SalePrice"
y <- data[[target]]
data1 <- data[, !colnames(data) %in% target]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_X <- as.matrix(data1[train_indices, ])
train_Y <- y[train_indices]
test_X <- as.matrix(data1[-train_indices, ])
test_Y <- y[-train_indices]
```

Fitting the Random Forest Model and then visualising of feature importance.
```{r}
data.rf <- randomForest(train_Y ~ ., train_X, ntree= 100, keep.forest= TRUE, importance = TRUE)
varImpPlot(data.rf)
```

Predicting the SalePrice using the random forest model.
```{r}
predictions <- round(predict(data.rf, test_X),0)
```

Combining the predicted and observed sale price for comparison purposes. 
```{r}
comb <- data.frame(Pred = predictions, Obs = test_Y)
print(head(comb))
```

Calculating the performance measures that will be used for comparison purposes.
```{r}
sse <- sum((test_Y - predictions)^2)
ssto <- sum((test_Y - mean(test_Y))^2)
R2 <- round((1 - (sse / ssto))*100,0)
# r_squared <- round(cor(predictions, test_Y)^2 *100,0)
mse <- mean((test_Y - predictions)^2)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae / mean)*100,2)
cat("The Coeficient of Determination is: ", R2, "%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average value of the data is:",perc,"%", "\n")
```

To visualize the difference between the observed and predicted sale prices, a line plot can be generated. This plot will effectively illustrate the variance between the two variables.
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$Obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size
par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)
# Plot the line graph
plot(x, comb$Obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Random Forest Observed vs Predicted Prices")
# Add the line for predicted values
lines(x, comb$Pred, col = "red", lwd = 2)
# Add a legend
legend(x = "right",y= mean(comb$Obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))
```

Reduced Model (Using top40 variables)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To optimize the random forest model, a reduced version will be fitted using the top 40 variables (as ascertained by the PAC technique). These variables were carefully selected based on the %incNodePurity metric, which emphasizes the quality of split points in the decision tree. Through experimentation, it was discovered that reducing the number of variables further increased the mean absolute error and decreased the R^2 value. Consequently, the decision was made to utilize the top 40 variables, chosen based on their %incNodePurity scores. This approach was favored over using the %IncMSE variables, as they yielded inferior model performance.
```{r}
importance_scores <- importance(data.rf, type=2)
# Convert the matrix to a data frame and include the row names as a variable
importance_df <- as.data.frame(importance_scores) %>%
  rownames_to_column(var = "Feature")
# Sort the data frame by importance scores in descending order
importance_df <- importance_df[order(importance_df$IncNodePurity, decreasing = TRUE), ][1:40,]
```

Visualising the top40 features (recommended by the randomForest approach) ranked based on their level of importance in determining the selling price of houses.
```{r}
ggplot(importance_df, aes(y = reorder(Feature, IncNodePurity), x = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(y = "Variable Name", x = "IncNodePurity Score", title = "Top 40 Important Variables") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability
```

Putting the Top40 variables into a dataset that will be used to fit the reduced model.
```{r}
top40 <- data.frame(Variables = importance_df[1:40,1])
top40 <- data[, top40$Variables]
names(top40)
top40$SalePrice <- data$SalePrice
```

Split the data into training and testing sets.
```{r}
target <- "SalePrice"
y <- top40[[target]]
data1 <- top40[, !colnames(top40) %in% target]
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
train_X <- as.matrix(data1[train_indices, ])
train_Y <- y[train_indices]
test_X <- as.matrix(data1[-train_indices, ])
test_Y <- y[-train_indices]
```

Fitting the reduced random forest model.
```{r}
top40.rf <- randomForest(train_Y ~ ., train_X, ntree= 100, keep.forest= TRUE, importance = TRUE)
```

Predicting the selling price of houses using the reduced model.
```{r}
predictions <- round(predict(top40.rf, test_X),0)
```

Combining the predicted and observed sale price for comparison purposes. 
```{r}
comb <- data.frame(Pred = predictions, Obs = test_Y)
print(head(comb))
```

Calculating the performance measures that will be used for comparison purposes
```{r}
sse <- sum((test_Y - predictions)^2)
ssto <- sum((test_Y - mean(test_Y))^2)
R2 <- round((1 - (sse / ssto))*100,0) 
# r_squared <- round(cor(predictions, test_Y)^2 *100,0)
mse <- mean((test_Y - predictions)^2)
rmse <- sqrt(mean((test_Y - predictions)^2))
mae <- mean(abs(test_Y - predictions))
perc <- round((mae / mean)*100,2)
cat("The Coeficient of Determination is: ", R2,"%", "\n")
cat("The Mean Squared Error is: ", mse, "\n")
cat("The Root Mean Squared Error is: ", rmse, "\n")
cat("The Mean Absolute Error is: ", mae, "\n")
cat("The relative size of the MAE compared to the average SalePrice of the dataset is:",perc,"%", "\n")
```

Generating a line plot to visualize the difference between the observed and predicted sale price. 
```{r}
# Create a sequence of numbers for the x-axis
x <- seq_along(comb$Obs)
# Disable scientific notation on the y-axis
options(scipen = 999)
par(las = 2, # Adjust margin
    cex.axis = 0.6) # Adjust axis label size
par(mar = c(4, 4, 3, 5.6)) # c(bottom, left, top, right)
# Plot the line graph
plot(x, comb$Obs, type = "l", col = "blue", lwd = 2, xlab = "Index", ylab = "Sale Price", main = "Random Forest Observed vs Predicted Prices")
# Add the line for predicted values
lines(x, comb$Pred, col = "red", lwd = 2)
# Add a legend
legend(x = "right",y= mean(comb$Obs), legend = c("Observed", "Predicted"), col = c("blue", "red"), lty = 1, lwd = 2,xpd=TRUE , cex = 0.7, inset = c(-0.2, 0))
```
